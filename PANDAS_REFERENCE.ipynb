{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "axis=0  across vertical  **  axis=1 across horizontal\n",
    "Creating NaN  -  np.nan\n",
    "\n",
    "type(NaN) is Float, so if we want to convert any object column with NaN to int, it throw an error because float(NaN) cant convert to int, so convert the column into Float using df['Col']astype(float)\n",
    "\n",
    "df.to_csv('df_csvformat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization setting "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_columns',85)\n",
    "pd.set_option('display.max_rows',85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Dummy DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame([('parrot',   24.0, 'second'),('monkey', np.nan, None)],\n",
    "                  columns=('name', 'max_speed', 'rank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']}\n",
    "pd.DataFrame.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# READING DIFFERENT DATA FORMATS TO PANDAS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv('sample_data.csv')\n",
    "\n",
    "df =pd.read_csv('sample_data.csv' , index_col=0) # this will omit the default index column creation by pandas\n",
    "\n",
    "df =pd.read_csv('sample_data.csv' , index_col=n)#this will bring nth column in the data set to the place of index\n",
    "\n",
    "df =pd.read_csv('sample_data.csv' , index_col=[n1,n2,n3])#this wil bring the columns n1,n2,n3 to first three colums\n",
    "\n",
    "#df =pd.read_csv('sample_data.csv' , index= ['first','second',....]) #this will create custome made index instead of defualt index values 1,2,...\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_excel = pd.read_excel(sample_data.xlsx')  #excel format\n",
    "                         \n",
    "df_txt = pd.read_csv('sample_data.txt',delimiter ='\\t')  #text format , with delimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSING /Summary statistics/ Function of the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Frame analysis3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.shape\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.head(3) / df.tail(3)\n",
    "\n",
    "df.index\n",
    "\n",
    "AGGREGATE ELEMENTS\n",
    "\n",
    "df.describe()   #aggregates(median,min,max,std,mean,count) for all the columns with numeric values\n",
    "df.median()     #50% percentile \n",
    "df.mean()   \n",
    "dd.max()\n",
    "df.min()\n",
    "df.std()\n",
    "df.count()         #gives count of all values except NaN\n",
    "df.size\n",
    "df['Col'].size()  #couunt with nan values in a columns or series\n",
    "df.quantile(0.50)  #calculating the quantiles/percentiles for all the int df columns\n",
    "\n",
    "dfc['Age'].rolling(2).mean()  #get the rolling avg for every 2,  dfc['Age'].rolling(5).mean() #for every 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysing as columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['Col_name'].unique()  #get all the unique values in that column\n",
    "\n",
    "df['Col_1'].head()       #first 5 values of a specific columnn\n",
    "\n",
    "\n",
    "df['col_1'].describe()   #aggregates(median,min,max,std,mean,count) for specific column\n",
    "df['col_1'].median()\n",
    "df['col_1'].mean()   \n",
    "df['col_1'].max() \n",
    "df['col_1'].min()\n",
    "df['col_1'].std()\n",
    "df['col_1'].count()\n",
    "df['col_1'].sum() # sum can be used with boolean filter al well, it will give the count of true values\n",
    "df['col_1'].unique() #give the unique values in the column col_1\n",
    " \n",
    "df['Col_1'].quantile(0.25)  #percentile in a specific col with int values\n",
    "\n",
    "df[['col_1','col_2']].info()   #if want the info of one col, it wont because info not available in series, so add one more col to make it into 2d df and get the info of required col_1\n",
    "\n",
    "\n",
    "df.T     #transposing DF\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "THESE ALL CAN BE INSERTED IN TO A DICTIONAY\n",
    "\n",
    "p=dict(df.max())\n",
    "\n",
    "p=dict(df['Col_1'].describe())\n",
    "\n",
    "p=dict(df.describe())\n",
    "p['col_1') to get the details of one columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To find the dependencies/relations  among the features use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cor=df.corr()  #to get the correlation matrix  and find if any two or more features are highly correlated or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= df_cor[\"Col-1\"].sort_values(ascending=False)\n",
    "#Gives the correleation of all the features against 'Col-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below code will give the heatmap for correlation or simply sns.heatmap(cor)\n",
    "plt.figure(figsize=(12,10))\n",
    "cor=X_train.corr()\n",
    "sns.heatmap(cor,annot=True,cmap=plt.cm.CMRmap_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "to avoid dataleakange and overfitting , we only apply the coreation check with train data\n",
    "and in case if we drop any features, then drop the same from test as well\n",
    "In general the threshold value is .85 or 85%, the columns with corrleation above 90 % , we will drop one of the columns\n",
    "Also negative correlated columns/features should not be removed "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# below code can be used to check the correlation between the columns\n",
    "def correlation(dataset,threshold):\n",
    "    col_corr=set()\n",
    "    corr_matrix=dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i,j] >threshold):  #thois abs will give -ve correated columns also , so remove abs from code\n",
    "                colname=corr_matrix.columns[1]\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "cor_features =correlation(X_train,0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAX and MIN across Row /Col "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.max(axis=0, skipna=None, level=None, numeric_only=None/True, **kwargs)  #across columns\n",
    "df.max(axis=1, skipna=None, level=None, numeric_only=None/True, **kwargs)  #across rows\n",
    "df[['col','col2'..]].max(axis=0, skipna=None, level=None, numeric_only=None/True, **kwargs)\n",
    "\n",
    "df.min(axis=0, skipna=None, level=None, numeric_only=None/True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing with SNS for finding where all are the NaN values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE INDEX "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.index  #give the index values info in a list, in case index is not a continiouse integer\n",
    "df.index[2]  - to get the index value in case index series is an object/string,inside baracket is position of the index in row\n",
    "\n",
    "df['original_index']=df.index  #this will preserve the original index  into a new column original index, incase if we want to make else column into index\n",
    "\n",
    "df.set_index(column_name, inplace=True)    # set a specific column into index\n",
    " \n",
    "df.sort_index(inplace=True)  # sort the df by  index values\n",
    "\n",
    "df.reset_index(inplace=True)   # reset the index to a continuous number , \n",
    "                               in case the index is an object/string then it will create a \n",
    "                                condinuious numeric index column as index and \n",
    "                                change the existing object/string index to a col\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order the Columns in Alphabetical or asecnding values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.sort_index(axis= 1,ascending= True)    #Column headings will get sorted ,axis =1 means horizontal dir, ie columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATING COLUMN NAMES FOR ANALYSIS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.columns=['Res','MainB','Hobby','OpenSourcer']  #comnverting all the column names, but no practical use\n",
    "# THE LIST SHOULD PASS ALL THE COLUMNS NAMES EVEN NO NEED TO CHANGE "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " df.rename(columns={'col_1': 'newCol_1', 'col_2':'NewCol_2'} ,inplace=True)   #renaming specific column"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2.columns =[x.lower() for x in df2.columns]  # modifying the column names\n",
    "\n",
    "x.upper()  #conver to lwr case\n",
    "s.capitalize() #Letter Case\n",
    "x.swapcase()  #swap upp and lwr case\n",
    "x.title()  #converting to title form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to check"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  df.columns=df.columns.str_replace(' ','_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FILTERING METHODS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTERING COLUMNS AS PRIMARY ELEMENT"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['col_1']  #select whole row in a column\n",
    "\n",
    "df[['Col_1','Col_2','Col_3']] #select whole row in a mutiple columns\n",
    "\n",
    "df[['col_1','Col_2','Col_3']][1:5]  #selected rows in columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FILTERING ROWS AS PRIMARY ELEMENT ILOC /LOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ILOC -integer loc"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.iloc[0:5]   # whole data in selected row in axis =1\n",
    "\n",
    "df.iloc[0:5, 1:3 ]   # data in selected row and selected columns with index\n",
    "\n",
    "df.iloc[0:5]['col_3']  \n",
    "\n",
    "df.iloc[0:5] [['Col_1','col_2','Col_4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LOC-Filtering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.loc[startindexname: endindexname , 'Col_1': 'Col_5' ]              #selecting range of columns \n",
    "\n",
    "df.loc[startindexname :endindexname , ['Col_1','Col_2','Col_3']]      #list of columns to select\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADVANCE FILTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boolean Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean Operations for above filter\n",
    "\n",
    "filt =(df['Col_1']>10) | (df['Col_3']<15)   #OR\n",
    "\n",
    "filt =(df['Col_1'] !=10) & (df['Col_3']== 15)   #AND"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#combining filter with LOC\n",
    "\n",
    "filt = ['Col_1','Col_2','Col_3'] \n",
    "\n",
    "df.loc[startindexname :endindexname , filt]   #adding filt,if nto range but ransom list of column names  \n",
    "\n",
    "df.loc[:, filt]  # whole range of index or everything in axis=0\n",
    "\n",
    "STEP FILTERING\n",
    "\n",
    "\n",
    "Filt1 = df['Col_1'] == 10\n",
    "Filt2 = df['Col_5'] == 'YEs'\n",
    " \n",
    "df.loc[filt1 ,['Col_1':'Col_3']]     #adding range of columns to get in iloc or  \n",
    "\n",
    "df.loc[Filt1]   #from the entire df the rows with filt1 ==True \n",
    "\n",
    "df.loc[0:10][filt1]  #first select the range 0 to 10, then apply filt1 within that range of data\n",
    "\n",
    "df.loc[filt1]    #applying filter directly instead of index range to whole df with rows meeting filt values get selected\n",
    "\n",
    "\n",
    "df.loc[filt1][filt2]    #step filtering with multiple filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search with String in a col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search with String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt= df['Col_2'].str.contains ('XXXX', na=False)  #this will give the string from Col_2, \n",
    "#for example in titanic df['Name'].str.contains('Dr') , will give all the names and data with Doctor\n",
    "df.loc[filt]\n",
    "\n",
    "filt=df['col']\n",
    "df.loc[filt]['col_2'].str.contains('xxxx',na=False)\n",
    "\n",
    "#Note=This string.contains apply only to one columsn, so we have to filter out one column first before applying this function to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEARCH WITH WHOLE STRING (ISIN METHOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt=df['Col_1'].isin(['two','four']) \n",
    "df[filt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search with Null or notnull rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt3=df['Cabin'].notnull()\n",
    "df.loc[filt3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### SELECTING THE COLUMNS WITH STRING OR FLOAT OR INT ONLY AS DATA"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.select_dtypes(include=['integer'])  #this will give the whole df with integer values only\n",
    "\n",
    "df.select_dtypes(include=['integer']).columns  #from above code it will return only the columns names \n",
    "\n",
    "df.select_dtypes(include=['integer','float']).columns\n",
    "\n",
    "df.select_dtypes(exclude = ['object']).columns  # all except object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in dfc.columns:\n",
    "    if [dfc[col].dtypes =='object']:\n",
    "         print(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IX INDEXING (COMBINATION OF ILOC AND LOC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA CLENSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop / HANDLING NaN and Null VALUES "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filt=df1['col_1'].isna() # boolean for the NaN values in column col_1\n",
    "filt=df1['col_1'].isnull() # boolean for the NULL values in column col_1\n",
    "filt =df.isna().any/all(axis=1)  # boolean for the NaN values in DF , index is true if any/all nan value present in row wise\n",
    "filt =df.isnull().any/all(axis=1)  # boolean for the NULL values in DF\n",
    "\n",
    "filt =df.isna().any/all(axis=0)  # boolean for the NaN values in DF , colum name s true if any/all nan value present in that cole\n",
    "filt =df.isnull().any/all(axis=0)  # boolean for the NULL values in DF\n",
    "\n",
    "df1.loc[filt]   #apply above four filters to get the rows of nan/Null values in DF \n",
    "\n",
    "#DROPING THE ROWS IN THE COL col WITH NA VALUE\n",
    "filt1=df['col'].isna()\n",
    "index=df[filt1].index\n",
    "df.drop(index=df[filt].index,inplace=True)\n",
    "\n",
    "df=df.dropna() #below is the default value of this option\n",
    "df=df.dropna(axis='index' how ='any')   #will drop that rows in case any one of the cell has Nan value\n",
    "df=df.dropn(axis='index', how ='all')   #will drop that row only if all the values in that row is Nan\n",
    "\n",
    "df =df.dropn(axis='columns' ,how='any' /'all') #this will look through the column\n",
    "\n",
    "\n",
    "df=df.dropna(axis ='index',how= 'any'/'all' ,subset =['col1','col2','col3'])\n",
    "df=df.dropna(axis ='index',how= 'any' ,subset =['col1'])\n",
    "#above subset is, if any or all of the columns in subset has Nan then drop that row\n",
    "\n",
    "\n",
    "#droping columns with na value filter \n",
    "filt=df['body'].isna()\n",
    "df.drop(index=df[filt].index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### isnull notnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt=df['Col_1'].isin(['two','four']) \n",
    "df[filt]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "NUL AND NOT NUL ROW SELECTION      not clear\n",
    "\n",
    "filt2=df['Col_1'].isnull()\n",
    "filt3=df['Col_1'].notnull()\n",
    "\n",
    "df.loc[filt1]\n",
    "df.loc[filt1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rows without NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[~filt1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GET THE COLUMNS WITH NULL  or Non Null VALUES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "COL_WITH_ MISSING_VALUES =  [col for col in df.columns if df[col].isnull().any()] #give the columns with null value\n",
    "\n",
    "COL =  [col for col in df.columns if df[col].notnull().all()]  #give list of columns without null value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Percentage of Null value in column"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['col_1'].isnull().sum()/len(df['col_1'])\n",
    "#df['col_1'].isnull().sum()/df['col_1'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    x=df[col].isnull().sum()/len(df[col])\n",
    "    print(col,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#can add a threshold value\n",
    "for i in df.columns:\n",
    "    perc=df[i].isnull().sum() / len(df[i])\n",
    "    if perc > .18:\n",
    "        print(i ,perc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fill NA Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('test/number /avg etc')\n",
    "df['col'].fillna('test/number /avg /median etc')\n",
    "#x=df['col_1'].mean()  # to get the avg value of the column\n",
    "\n",
    "df[col]=df[col].fillna(df[col].mean()[0]) #coverting the Na in col with mean value of the column (column is int value and not outliers)\n",
    "df[col]=df[col].fillna(df[col].median()[0]) #coverting the Na in col with mean value of the column (column is int value)\n",
    "df[col]=df[col].fillna(df[col].mode()[0]) #coverting the Na in col with Mode value of the column (column is object values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#automate the fillna the object OR int/float columns with  mode or mean\n",
    "\n",
    "for col in  df.columns:\n",
    "    if(df[col].dtype == np.int64) or (df[col].dtype == np.float) :\n",
    "        df[col]=df[col].fillna(df[col].mean())\n",
    "    else:\n",
    "        df[col]=df[col].fillna(df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### missing value total in each columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "missing_val_count_by_column = (df.isnull().sum()) \n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0]) #OR\n",
    "missing_val_count_by_column[missing_val_count_by_column/1460>.80]  #missing value >80%\n",
    "list(missing_val_count_by_column[missing_val_count_by_column/1460>.80.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing all string NA,no data  or Missing  with NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for uniqueness fill all unavailable data cell with one string eg:missing , then treat all the cell with missing accordignly\n",
    "df.fillna('missing') #to fill all the na alue with word missing\n",
    "df.replace('NA',np.nan,inplace=True)\n",
    "df.replace('MISSING',np.nan ,inplace=True)\n",
    "#by using unique we can find the missing,NA or not entered , string vlaues and replace it with above "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note that np.nan is a a float data type , can be check with below code\n",
    "type(np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace all the values in row or col"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['col_1']=a  # replace all the values in col_1 with a\n",
    "\n",
    "df.iloc[2] =['A','B',5,'d'] #replace all the values in row with new value, but have to pass all the cell values in that row  rarely used\n",
    "df.iloc[2] ='A'   # replace all the values with A  rarely used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace values in a specific cell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.iloc[2,5] = 'a' #replace the value in specific cell\n",
    "#or\n",
    "df.loc[2, ['col name']] =A\n",
    "#or we can use filter to select the cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMOVING WHITE SPACE FROM THE SERIES DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_1']=pd.core.strings.str_strip(df['COL_1']) #here we need to assign to original column, no inplace command "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting Data types into another Data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col'] =df['col'].astype(int)\n",
    "df['col']=df['col'].astype(float)\n",
    "df['col' =df['col'].astype(object)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if there is a np.nan present then that column cannot convert into int . because of nan (float)\n",
    "so replace the nan with zero and then convert it into int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SORTING"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.sort_values(by='COL_1')                                      #Sorting the DF wrt the values in Col_1\n",
    "df.sort_values(by='Country',ascending =False ,inplace=True)     #above same with descending order\n",
    "\n",
    "df.sort_values(by=['Col_1','Col_2'] )   #sort by Col_1  first, if Col_1 have equal values then  consider Col_2 values to sort\n",
    "df.sort_values(by=['Col_1','Col_2'],ascending=[True,False] ) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['Col_1'].sort_values()        #get sorted col_1 ascending or descending\n",
    "df.sort_values(by='Col_2')['Col_1']   # get arrange col_1 wrt the sorted values in Col_ 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['col_1'].sort_values(ascending=False)[5]  #gets the 6th largest from top in that column\n",
    "                                             #this can be used for filtering also"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DF back to original index sort "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.sort_index()  \n",
    "df.sort_index(axis=0, ascending=True)    \n",
    "df.sort_index(axis= 1,ascending= True)    #Column headings will get sorted ,axis =1 means horizontal dir, ie columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COUNTS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df['Col_1'].count()  # gives the total Non-NaN values in col_1 ,its 1 of the aggregate in describ\n",
    "\n",
    "len(df['Col_1'] )  # gives all include NaN value or we can use  - df['Col_1'].size\n",
    "\n",
    "df['Col_1'].value_counts()  #gives count of each values inside the col_1\n",
    "\n",
    "df['Col_1'].value_counts(normalize=True)  #gives percentage count of each values inside the col_1\n",
    "\n",
    "d= dict(df['Col_1'].value_counts()) #Take the output as a dictionary "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Count of specific value in a column"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filt1= df['Col_1'] > 10  #filtering values >10\n",
    "filt2 =df['col_1'] < 10  #filtering values <10\n",
    "\n",
    "x=len(df[filt1])  # counting the length of the rows with filt1 condition ,(get the count of male/female in sex columns)\n",
    "y=len(df[filt2])  #counting the length of the rows of filt1\n",
    "\n",
    "df[filt1]  # getting df with only filt1 condition satisfies     \n",
    "df[filt2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "An example df with which cars people prefere in different countries DF col is  country , person name, carname\n",
    "TO FIND MOST popular prefered car in India\n",
    "\n",
    "filt1=df['Country'] =='India'\n",
    "\n",
    "df.loc[filt1]['car'].value_counts() # gives the highset to lowest of the cars counts in india , 10 people prefer Hyundai, 8 prefer suzuki etc\n",
    "\n",
    "#get the output in the form of a dict d=dict(df.loc[filt1]['car'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.unique()  #give the unique counts in whole df wrt columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADDING DATA FRAMES   TO EACH OTHER"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "merged=pd.concat([df,df1],axis=1)  #concantinating two df in horizontal direction \n",
    "merged=pd.concat([df,df1],axis=0)  #concantinating two df in vertical direction, if any column name in df2 name matching with any column in df, then the data will add below that column and remaining cell fill be Nan\n",
    "\n",
    "df.append(df2,ignore_index=True)     # appending a df2 to df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Row or Column in Data Farame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NewCol']=0    #adding dummy columns with a constant\n",
    "df['NewCol'] =df['Col1'] + 10   #the col1 should be float or int\n",
    "df['Newcol'] = df['col_1'] +df['col_2'] \n",
    "df.append({\"Name\":\"Trivandrum\"},ignore_index=True)   # add a new row and add 'Trivandrum' in Name column and remaining col with NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SPLITING EXISITNG COLUMNS with delimiter and create new columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "splitcol= df['col_1'].str.split('_') # gives a single column with the strings in col_1 splited \n",
    "splitcol= df['col_1'].srt.splot('_', expand=True)  # the two splited columns will be asigned to seperate columns\n",
    "\n",
    "df[['Newcol_1', 'NewCol_2']] =splitcol  #newly splited columns will b assigned to the new columns Newcol_1 & NewCol_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOIN TWO DATAFRMAES note below link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.datasciencemadesimple.com/join-merge-data-frames-pandas-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEL ROWS and COLUMNS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.drop(columns=['col_1'] , inplace=True)    # delete single column\n",
    "df.drop(columns =['col_1','Col_2'], inplace=True)  #deleting multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Row wise Deletion"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.drop(index=10)   #DROP BY INDEX "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filt=df['OpenSourcer']==\"Never\"    #Drop rows by filter\n",
    "\n",
    "df.drop(index=df[filt].index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "filt2=df.iloc[-6:]                         #droping by iloc /loc\n",
    "\n",
    "df.drop(filt2.index,inplace=True)  # droping last six rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deleting duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGULAR EXPRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacig Text with ReEx\n",
    "\n",
    "df['col_1'].replace(to_replace=r'[^a-zA-Z_]', value='_', regex=True)\n",
    "\n",
    "test=\"abce15abce__66@@@avvv\"\n",
    "test1/test =re.sub('[^a-zA-Z_]','',test) #replacing everything except a toz,A toZ  and _ with\"\"\n",
    "print(test1)\n",
    "test=re.sub('[a-zA-Z_]','',test)  ##replacing everything  a toz,A toZ  and _ with\"\"\n",
    "test=re.sub(r'[@()]','',test)  ##replacing @ ()_ with\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterrows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this can be used to iterate through the rows , used in For loop\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    print(index,row)\n",
    "\n",
    "#iterrows can be with specific column also\n",
    "for index ,row in df.iterrows():\n",
    "    print(index,row['Name'])  #only that column only will get iterated\n",
    "    \n",
    "for index ,row in df.iterrows(): #in case we have column for price and quantity\n",
    "    pri=(row['Price'])\n",
    "    qty=(row['Quantiy'])\n",
    "    price/quantity =pri /qty\n",
    "    \n",
    "for index, row in df.iterrows():\n",
    "    print(index, row['Name':'Age'])  #it will print cells from Name upto cell Age in iteration\n",
    "for index, row in df.iterrows():\n",
    "    print(index, row[['Name','Age']])  #it will print cells Name &'HP' in iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMALLEST AND LARGEST "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "   \n",
    "\n",
    "df.nlargest(3,'Col_1')  #Whole df first three rows wrt 3 largest in col_1\n",
    "\n",
    "df.nlargest(1,'Col_1')['Col_2']  #wrt above condition gives only the value of specific columns-col_2\n",
    "\n",
    "\n",
    "\n",
    "df['Col_name'].nlargest(5)  #five largest in a specific column \n",
    "\n",
    "df['Col_name'].nsmallest(5)  #five smallest in a specific column\n",
    "\n",
    "(df.nlargest(5,'Col_name'))   #gives the whole DB with 5 largest values in col_name\n",
    "\n",
    "#can get as a dictionary\n",
    "\n",
    "\n",
    "df.idxmax(axis = 0/1, skipna = True)  #gives the index of the max value in all columns in df\n",
    "df['col_1'].idxmax(axis-=0,skipna=True)   # give the index of max value in col_1\n",
    "\n",
    "df['col_1'].idxmin(axis=0 ,skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idxmax ,idxmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_max=df['col_1'].idxmax() #returns the index of the max value in col_1\n",
    "index_max =df['col_1'].idxmin() #returns the index of the min value in col_1\n",
    "\n",
    "\n",
    "above filter can be combined with loc to get the max value also\n",
    "cell_val=df.loc[index_max]['Col_1']  #access the value with index value  ,accessing with loc and index value\n",
    "\n",
    "df['col_1'].max()  #gives the max value in col_1 , refer column -analysis for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_Embarked=df.groupby(['Embarked'])\n",
    "#above command will take all the unique values in embarked col and split the df_vertically wrt the number of unique value \n",
    "#Number of split equeal to the number of unique values\n",
    "#Each unique value act as a temp index for the newly created temp.df  for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_Embarked.get_group('C')  #This can be done with  boolean filter  Embarked=='C'  \n",
    "#but groupby can already split and ready the data for further anaylysis , \n",
    "#but filter onlye group one element in the filter in our case C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_Embarked['Gender'].value_counts()  #we can see the group will act as a temperory index , so we can apply loc over it"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#above can be done by loc but only for one set at atime\n",
    "\n",
    "filt=df['Embarked']=='C'\n",
    "df.loc[filt]   \n",
    "df.loc[filt]['Gender']    #apply column filter\n",
    "df.loc[filt]['Gender'].value_counts()    #apply value_counts on the selected column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_Embarked['Gender'].value_counts().loc['C']\n",
    "group_Embarked['Gender'].value_counts(normalize=True).loc['C']  #for percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_Embarked['Fare'].median()\n",
    "group_Embarked['Fare'].mean()\n",
    "group_Embarked['Fare'].mean().loc['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we need more than one aggregate  use agg function \n",
    "group_Embarked['Fare'].agg(['mean','median'])\n",
    "#emb['Fare'].agg(['mean','median']).loc['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many doctors\n",
    "filt=df['Name'].str.contains('Dr')\n",
    "df.loc[filt]['Name'].count()\n",
    "\n",
    "#Apply over groupby\n",
    "\n",
    "group_Embarked=df.groupby(['Embarked'])\n",
    "group_Embarked['Name'].apply(lambda x: x.str.contains('Dr').sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Groupby for multiple columns\n",
    "\n",
    "group=df.groupby(['Gender','Embarked'])\n",
    "group['Fare'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply Method in Pandas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Apply, picks each element at a time from a collection of data and  applies a function or  recomonded modification\n",
    "on each element and return it\n",
    "For example if we pass a series it will take indivitual cell element to apply the feature \n",
    "            If we pass a dataframe it will take each column from the Df to appy the feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for example in Titanic Data below i want to check the length of each name and assign to a new column\n",
    "\n",
    "df['Len_name']=df['Name'].apply(len)\n",
    "\n",
    "#Want to add an extension to every name \n",
    "\n",
    "df['New_name'] =df['Name'].apply(lambda x :x + 'Addition')\n",
    "\n",
    "#want to add a value to the integer\n",
    "\n",
    "df['New_age']=df['Age'].apply(lambda x :x +20)\n",
    "\n",
    "#to get the first element from the name\n",
    "\n",
    "df['surname']=df['Name'].str.split(' ').apply(lambda x: x[0]) #Split will convert each name into a list ,seperated with space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col1'].apply(sum)  #col1 shoubl be int or float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age'].apply(lambda x :x+5)\n",
    "df[['Age','Fare','Survived','Pclass']].apply(lambda x :x+5)  #all the col should be int or float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.apply(sum,axis=0) #give sum in columns direction\n",
    "df2.apply(sum,axis=1) #give sum in row direction\n",
    "#Alll the column should be in int or float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get the count of Dr from each Embarked point\n",
    "group_Embarked=df.groupby(['Embarked'])\n",
    "group_Embarked['Name'].apply(lambda x: x.str.contains('Dr').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PIVOT TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suppose have data with flight passengers in a country, data in month, date in year and number of passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot =df.pivot_table(index='month',columns='year',values='passengers')\n",
    "will give a table as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fdf=sns.load_dataset('flights') #importing a seaborn data\n",
    "pivot =fdf.pivot_table(index='month',columns='year',values='passengers')\n",
    "pivot\n",
    "#convert this into a heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Catagorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Dummies  -One Hot Encoding Using Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_col_df=pd.get_dummies(df['col']) #Label encoded all the class \n",
    "New_col_df=pd.get_dummies(df['col'],drop_first=True)  # if we want to drop first column\n",
    "New_col_df=pd.get_dummies(df['col'],drop_first=True,dummy_na=True) # if we want to include Nan as well"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "New-_col_df will create a dataframe with columns have unique values in df['col'], and drop_first will drop the first class\n",
    "For an example in df['Grade'] catagorical column has class A+,A,B+,B so in new_col_col will have columns A,B+ and B only and drop the col with A+\n",
    "\n",
    "And this newly created dataframe can get concantiated with the X using pd.Concat axis=1(code below) function  and delete the df['Grade'] from X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.concat([df,New_col],axis=1)  #concatinating the encoded dataframe to original df\n",
    "df.drop(columns=['Col'],inplace=True)  #delete the encoded column(col) from the original df\n",
    "#then data frame is ready to fed to the machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE-\n",
    "Same one hot encoding should be applied to test/prediction data before prediction\n",
    ",other wise the feature column between the train and test data mismatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we use \n",
    "df=pd.get-dummiers(df,drop_first=True,dummy_na=True) #This will convert all the object columsn into one hot encode\n",
    "#but this has less control on handling additional classes between test  and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Need to read about Variable trap in Models ????"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### In case if we have additional class in Test data or prediction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Problem:\n",
    "If we have additional class in test/predict data , then while applying OHE to test , it will have an additional \n",
    "feature column in compared to the train Feature.And throw and error, because our model was not trained\n",
    "for this additional feature in test/predict data.\n",
    "Solution:\n",
    "    1Find additional classes in the test data compared to train/predict.\n",
    "    2.Convert the feature in Train into catagorical data type ,adding the additional class from the \n",
    "    test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For exmaple if the test data has an additional grade 'C', which is not in train.\n",
    "This will create and additional column in train with 'C' =0 , during Get Dummies\n",
    "Code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.api.types import CategoricalDtype\n",
    "df['Grade']=df['Grade'].astype(CategoricalDtype(['A+','A','B+','B','C']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTE:\n",
    "If we have additional class in test/predict data, similar thing should be done in viseversa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Factorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['col'].factorize()   #will label encode with numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and empty Data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new=pd.DataFrame(columns=['Emptyrow'],axia=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CREATING DICT FROM TWO COLUMNS"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "lookup=dict(zip(df['Flower'].unique(),df['encoded_flower'].unique()))\n",
    "This can be applied ,when we have a catagorical output column and  label-encoded output feature column for training and prediction process.\n",
    "so we can use this dictionay to get the relation and create output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogram  /Bin numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc['Agegroup']=pd.cut(dfc['Age'],bins=(0,25,30),labels=['young','old'])\n",
    "#bins =0 is base limit, 30 upper limit. so any data outside the baseklimit and upper limit will considered as Nan,\n",
    "#then data between base and upper limit divivded into two with 25,so two classed\n",
    "#if want three classed then put two values between base and upper limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df['Col'] #should be an int/float\n",
    "hist=pd.cut(x,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy and Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to Data frame to Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_numpy(dtype=None, copy=False, na_value=<object object>)\n",
    "df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting 1dim to higher dim  n array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#to an excel\n",
    "df.to_excel(r'Path to store the exported excel file\\File Name.xlsx', index = False)\n",
    "#to an excel specific sheet\n",
    "df.to_excel(r'Path to store the exported excel file\\File Name.xlsx', sheet_name='Your sheet name', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing Outliers for Ploting"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "removing the outliers from lotarea vs saleprice by taking lotarea removing above .95 quantile\n",
    "we can try changing this from 99 to backward to check or use a boxplot to see or quantile formula\n",
    "to decide the outliers from the data\n",
    "\n",
    "filt =df['LotArea']<df['LotArea'].quantile(0.96)\n",
    "df_sub=df.loc[filt]\n",
    "\n",
    "sns.regplot(x='LotArea',y='SalePrice',data=df_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df['log_col'] = np.log(df['col'])    #LOG transformation\n",
    "\n",
    "\n",
    "df['col_squareroot']=np.sqrt((df['Col']))  #square root transformation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
